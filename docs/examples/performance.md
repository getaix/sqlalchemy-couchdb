# æ€§èƒ½åŸºå‡†æµ‹è¯•ç¤ºä¾‹

## æ¦‚è¿°

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ SQLAlchemy CouchDB æ–¹è¨€è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚

## åŸºå‡†æµ‹è¯•å¥—ä»¶

### 1. CRUD æ€§èƒ½æµ‹è¯•

```python
import time
from contextlib import contextmanager
from sqlalchemy import create_engine, text

@contextmanager
def timer(name):
    """è®¡æ—¶å™¨"""
    start = time.time()
    yield
    elapsed = time.time() - start
    print(f"{name}: {elapsed:.3f}s")

def benchmark_crud(engine, num_ops=1000):
    """CRUD æ“ä½œåŸºå‡†æµ‹è¯•"""
    print(f"\n{'='*60}")
    print(f"CRUD æ€§èƒ½åŸºå‡†æµ‹è¯• - {num_ops} æ“ä½œ")
    print(f"{'='*60}")

    # INSERT æµ‹è¯•
    print("\n1. INSERT æµ‹è¯•")
    with timer(f"æ’å…¥ {num_ops} æ¡è®°å½•"):
        with engine.connect() as conn:
            data = [
                {'id': f'bench:insert:{i}', 'name': f'User{i}', 'age': i % 100, 'type': 'user'}
                for i in range(num_ops)
            ]
            conn.execute(text("""
                INSERT INTO users (_id, name, age, type)
                VALUES (:id, :name, :age, 'user')
            """), data)
            conn.commit()

    insert_throughput = num_ops / elapsed
    print(f"æ’å…¥ååé‡: {insert_throughput:.1f} docs/s")

    # SELECT æµ‹è¯•
    print("\n2. SELECT æµ‹è¯•")
    select_count = min(num_ops, 1000)  # é™åˆ¶æŸ¥è¯¢æ•°é‡
    with timer(f"æŸ¥è¯¢ {select_count} æ¬¡"):
        with engine.connect() as conn:
            for i in range(select_count):
                result = conn.execute(text("""
                    SELECT * FROM users WHERE _id = :id
                """), {'id': f'bench:insert:{i}'})

    select_throughput = select_count / elapsed
    print(f"æŸ¥è¯¢ååé‡: {select_throughput:.1f} ops/s")

    # UPDATE æµ‹è¯•
    print("\n3. UPDATE æµ‹è¯•")
    update_count = min(num_ops, 1000)
    with timer(f"æ›´æ–° {update_count} æ¡è®°å½•"):
        with engine.connect() as conn:
            for i in range(update_count):
                conn.execute(text("""
                    UPDATE users
                    SET age = age + 1
                    WHERE _id = :id AND type = 'user'
                """), {'id': f'bench:insert:{i}'})
            conn.commit()

    update_throughput = update_count / elapsed
    print(f"æ›´æ–°ååé‡: {update_throughput:.1f} ops/s")

    # DELETE æµ‹è¯•
    print("\n4. DELETE æµ‹è¯•")
    delete_count = min(num_ops, 1000)
    with timer(f"åˆ é™¤ {delete_count} æ¡è®°å½•"):
        with engine.connect() as conn:
            for i in range(delete_count):
                conn.execute(text("""
                    DELETE FROM users
                    WHERE _id = :id AND type = 'user'
                """), {'id': f'bench:delete:{i}'})

    delete_throughput = delete_count / elapsed
    print(f"åˆ é™¤ååé‡: {delete_throughput:.1f} ops/s")

    return {
        'insert': insert_throughput,
        'select': select_throughput,
        'update': update_throughput,
        'delete': delete_throughput
    }

# è¿è¡ŒåŸºå‡†æµ‹è¯•
results = benchmark_crud(engine, num_ops=1000)
```

### 2. å¹¶å‘æ€§èƒ½æµ‹è¯•

```python
import concurrent.futures
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine

def concurrent_select_test(engine, num_threads=10, queries_per_thread=100):
    """å¹¶å‘æŸ¥è¯¢æ€§èƒ½æµ‹è¯•"""
    print(f"\n{'='*60}")
    print(f"å¹¶å‘æŸ¥è¯¢æµ‹è¯• - {num_threads} çº¿ç¨‹ï¼Œæ¯çº¿ç¨‹ {queries_per_thread} æŸ¥è¯¢")
    print(f"{'='*60}")

    def query_task(thread_id):
        """å•ä¸ªçº¿ç¨‹çš„æŸ¥è¯¢ä»»åŠ¡"""
        count = 0
        with engine.connect() as conn:
            for i in range(queries_per_thread):
                result = conn.execute(text("""
                    SELECT COUNT(*) as c FROM users WHERE type = 'user'
                """))
                count += result.fetchone().c
        return count

    start = time.time()
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [executor.submit(query_task, i) for i in range(num_threads)]
        results = [f.result() for f in concurrent.futures.as_completed(futures)]
    elapsed = time.time() - start

    total_queries = num_threads * queries_per_thread
    throughput = total_queries / elapsed

    print(f"å¹¶å‘æŸ¥è¯¢å®Œæˆ:")
    print(f"  æ€»æŸ¥è¯¢æ•°: {total_queries}")
    print(f"  æ€»è€—æ—¶: {elapsed:.3f}s")
    print(f"  ååé‡: {throughput:.1f} queries/s")
    print(f"  å¹³å‡æ¯ä¸ªæŸ¥è¯¢: {elapsed/total_queries*1000:.3f}ms")

    return throughput

# è¿è¡Œå¹¶å‘æµ‹è¯•
concurrent_results = concurrent_select_test(engine, num_threads=10, queries_per_thread=100)
```

### 3. æ‰¹é‡æ“ä½œæ€§èƒ½æµ‹è¯•

```python
def batch_operations_test(engine):
    """æ‰¹é‡æ“ä½œæ€§èƒ½æµ‹è¯•"""
    print(f"\n{'='*60}")
    print(f"æ‰¹é‡æ“ä½œæ€§èƒ½æµ‹è¯•")
    print(f"{'='*60}")

    # å•æ¡æ’å…¥ vs æ‰¹é‡æ’å…¥
    batch_sizes = [1, 10, 50, 100, 500, 1000]

    for batch_size in batch_sizes:
        print(f"\næ‰¹é‡å¤§å°: {batch_size}")

        # å‡†å¤‡æ•°æ®
        data = [
            {'id': f'batch:{batch_size}:{i}', 'name': f'User{i}', 'type': 'user'}
            for i in range(batch_size)
        ]

        # æ‰¹é‡æ’å…¥
        start = time.time()
        with engine.connect() as conn:
            conn.execute(text("""
                INSERT INTO users (_id, name, type)
                VALUES (:id, :name, 'user')
            """), data)
            conn.commit()
        elapsed = time.time() - start

        throughput = batch_size / elapsed
        avg_per_doc = elapsed / batch_size * 1000

        print(f"  è€—æ—¶: {elapsed:.3f}s")
        print(f"  ååé‡: {throughput:.1f} docs/s")
        print(f"  å¹³å‡æ¯ä¸ªæ–‡æ¡£: {avg_per_doc:.3f}ms")

# è¿è¡Œæ‰¹é‡æµ‹è¯•
batch_operations_test(engine)
```

### 4. å¼‚æ­¥æ€§èƒ½æµ‹è¯•

```python
async def async_performance_test(async_engine, num_ops=1000):
    """å¼‚æ­¥æ“ä½œæ€§èƒ½æµ‹è¯•"""
    print(f"\n{'='*60}")
    print(f"å¼‚æ­¥æ“ä½œæ€§èƒ½æµ‹è¯• - {num_ops} æ“ä½œ")
    print(f"{'='*60}")

    # å¼‚æ­¥æ‰¹é‡æ’å…¥
    print("\n1. å¼‚æ­¥æ‰¹é‡æ’å…¥")
    data = [
        {'id': f'async:insert:{i}', 'name': f'User{i}', 'type': 'user'}
        for i in range(num_ops)
    ]

    start = time.time()
    async with async_engine.connect() as conn:
        await conn.execute(text("""
            INSERT INTO users (_id, name, type)
            VALUES (:id, :name, 'user')
        """), data)
        await conn.commit()
    elapsed = time.time() - start

    insert_throughput = num_ops / elapsed
    print(f"å¼‚æ­¥æ’å…¥ååé‡: {insert_throughput:.1f} docs/s")

    # å¼‚æ­¥å¹¶å‘æŸ¥è¯¢
    print("\n2. å¼‚æ­¥å¹¶å‘æŸ¥è¯¢")
    query_count = min(num_ops, 1000)

    start = time.time()
    async with async_engine.connect() as conn:
        tasks = []
        for i in range(query_count):
            task = conn.execute(text("""
                SELECT * FROM users WHERE _id = :id
            """), {'id': f'async:insert:{i}'})
            tasks.append(task)

        results = await asyncio.gather(*tasks)
    elapsed = time.time() - start

    async_throughput = query_count / elapsed
    print(f"å¼‚æ­¥æŸ¥è¯¢ååé‡: {async_throughput:.1f} queries/s")

    return {
        'insert': insert_throughput,
        'select': async_throughput
    }

# è¿è¡Œå¼‚æ­¥æµ‹è¯•
async_engine = create_async_engine('couchdb+async://admin:password@localhost:5984/mydb')
async_results = asyncio.run(async_performance_test(async_engine, num_ops=5000))
```

### 5. å†…å­˜ä½¿ç”¨æµ‹è¯•

```python
import psutil
import os

def memory_usage_test(engine):
    """å†…å­˜ä½¿ç”¨æµ‹è¯•"""
    print(f"\n{'='*60}")
    print(f"å†…å­˜ä½¿ç”¨æµ‹è¯•")
    print(f"{'='*60}")

    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss / 1024 / 1024  # MB

    print(f"åˆå§‹å†…å­˜: {initial_memory:.2f} MB")

    # åŠ è½½å¤§é‡æ•°æ®
    with timer("åŠ è½½ 10000 æ¡è®°å½•"):
        with engine.connect() as conn:
            data = [
                {'id': f'mem:{i}', 'name': f'User{i}', 'age': i % 100, 'type': 'user'}
                for i in range(10000)
            ]
            conn.execute(text("""
                INSERT INTO users (_id, name, age, type)
                VALUES (:id, :name, :age, 'user')
            """), data)
            conn.commit()

    peak_memory = process.memory_info().rss / 1024 / 1024  # MB
    print(f"å³°å€¼å†…å­˜: {peak_memory:.2f} MB")
    print(f"å†…å­˜å¢é•¿: {peak_memory - initial_memory:.2f} MB")

    # æŸ¥è¯¢å¤§é‡æ•°æ®ï¼ˆæµ‹è¯•ç»“æœé›†å†…å­˜ï¼‰
    with timer("æŸ¥è¯¢ 10000 æ¡è®°å½•"):
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT * FROM users WHERE type = 'user' LIMIT 10000
            """))
            all_rows = result.fetchall()

    query_memory = process.memory_info().rss / 1024 / 1024  # MB
    print(f"æŸ¥è¯¢åå†…å­˜: {query_memory:.2f} MB")
    print(f"æŸ¥è¯¢å†…å­˜å¢é•¿: {query_memory - peak_memory:.2f} MB")

    # æ¸…ç†
    with timer("æ¸…ç†æ•°æ®"):
        with engine.connect() as conn:
            conn.execute(text("""
                DELETE FROM users WHERE _id LIKE 'mem:%'
            """))

    final_memory = process.memory_info().rss / 1024 / 1024  # MB
    print(f"æ¸…ç†åå†…å­˜: {final_memory:.2f} MB")

    return {
        'initial': initial_memory,
        'peak': peak_memory,
        'query': query_memory,
        'final': final_memory
    }

# è¿è¡Œå†…å­˜æµ‹è¯•
memory_results = memory_usage_test(engine)
```

### 6. å®Œæ•´åŸºå‡†æµ‹è¯•æŠ¥å‘Š

```python
def generate_benchmark_report(results):
    """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
    print(f"\n{'='*60}")
    print(f"æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
    print(f"{'='*60}")

    print("\nğŸ“Š CRUD æ“ä½œæ€§èƒ½:")
    print(f"  INSERT: {results['crud']['insert']:.1f} docs/s")
    print(f"  SELECT: {results['crud']['select']:.1f} ops/s")
    print(f"  UPDATE: {results['crud']['update']:.1f} ops/s")
    print(f"  DELETE: {results['crud']['delete']:.1f} ops/s")

    print("\nğŸš€ å¹¶å‘æŸ¥è¯¢:")
    print(f"  ååé‡: {results['concurrent']:.1f} queries/s")

    print("\nâš¡ å¼‚æ­¥æ“ä½œ:")
    print(f"  INSERT: {results['async']['insert']:.1f} docs/s")
    print(f"  SELECT: {results['async']['select']:.1f} queries/s")

    print("\nğŸ’¾ å†…å­˜ä½¿ç”¨:")
    print(f"  åˆå§‹: {results['memory']['initial']:.2f} MB")
    print(f"  å³°å€¼: {results['memory']['peak']:.2f} MB")
    print(f"  æŸ¥è¯¢å: {results['memory']['query']:.2f} MB")

    print(f"\n{'='*60}")
    print(f"åŸºå‡†æµ‹è¯•å®Œæˆ")
    print(f"{'='*60}")

# è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•
def run_full_benchmark():
    """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""
    engine = create_engine('couchdb://admin:password@localhost:5984/mydb')

    results = {}

    # CRUD æµ‹è¯•
    results['crud'] = benchmark_crud(engine, num_ops=1000)

    # å¹¶å‘æµ‹è¯•
    results['concurrent'] = concurrent_select_test(engine, num_threads=10, queries_per_thread=100)

    # å¼‚æ­¥æµ‹è¯•
    results['async'] = asyncio.run(async_performance_test(async_engine, num_ops=1000))

    # å†…å­˜æµ‹è¯•
    results['memory'] = memory_usage_test(engine)

    # ç”ŸæˆæŠ¥å‘Š
    generate_benchmark_report(results)

# è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•
run_full_benchmark()
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

åŸºäºåŸºå‡†æµ‹è¯•ç»“æœï¼š

### 1. æ’å…¥ä¼˜åŒ–

- **æ‰¹é‡æ’å…¥**ï¼šæ¯”å•æ¡æ’å…¥å¿« 10-100 å€
- **æ¨èæ‰¹é‡å¤§å°**ï¼š100-1000 æ¡è®°å½•
- **æ›´å¤§æ‰¹é‡**ï¼šæ”¶ç›Šé€’å‡ï¼Œå¯èƒ½å¯¼è‡´è¶…æ—¶

### 2. æŸ¥è¯¢ä¼˜åŒ–

- **ä½¿ç”¨ç´¢å¼•**ï¼šORDER BY å­—æ®µå¿…é¡»åˆ›å»ºç´¢å¼•
- **é™åˆ¶ç»“æœé›†**ï¼šå§‹ç»ˆä½¿ç”¨ LIMIT
- **åªæŸ¥è¯¢å¿…è¦å­—æ®µ**ï¼šå‡å°‘æ•°æ®ä¼ è¾“é‡

### 3. å¹¶å‘ä¼˜åŒ–

- **è¿æ¥æ± å¤§å°**ï¼šåŸºäºå¹¶å‘ç”¨æˆ·æ•°è°ƒæ•´ï¼ˆé€šå¸¸ 10-50ï¼‰
- **çº¿ç¨‹æ•°**ï¼šä¸è¶…è¿‡ CPU æ ¸å¿ƒæ•°çš„ 2-4 å€
- **å¼‚æ­¥æ“ä½œ**ï¼šé€‚åˆ I/O å¯†é›†å‹ä»»åŠ¡

### 4. å†…å­˜ä¼˜åŒ–

- **åˆ†é¡µæŸ¥è¯¢**ï¼šé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§é‡æ•°æ®
- **æµå¼å¤„ç†**ï¼šä½¿ç”¨ fetchmany() åˆ†æ‰¹è·å–
- **åŠæ—¶æ¸…ç†**ï¼šåˆ é™¤ä¸éœ€è¦çš„æ•°æ®

## ä¸‹ä¸€æ­¥

- [é«˜çº§ç‰¹æ€§ç¤ºä¾‹](advanced-features.md)
- [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](../dev/performance.md)
- [æµ‹è¯•æŒ‡å—](../dev/testing.md)
